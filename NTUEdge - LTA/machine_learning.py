# -*- coding: utf-8 -*-
"""machine learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LznVQfKOf8w4bkTjUMWO_63KyDYv9_Iq
"""

pip install ruptures

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sb
import scipy.fftpack
#from scipy.fft import fft, fftfreq

from sklearn import model_selection
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.tree import export_graphviz
from sklearn.utils import shuffle
from sklearn.naive_bayes import GaussianNB

#Import all csv files from github
import requests
url='https://raw.githubusercontent.com/kaiyang7766/ExploratoryDataAnalysis/main/Load_Data/LoadData.py'
r=requests.get(url)
with open('LoadData.py','w') as f:
  f.write(r.text)

import LoadData as LD
#List of dataframes imported:
bukitPanjangToExpo_s6edge=LD.bukitPanjangToExpo_s6edge
bukitPanjangToExpo_iphone12pro=LD.bukitPanjangToExpo_iphone12pro

#Import all functions from github
url='https://raw.githubusercontent.com/kaiyang7766/ExploratoryDataAnalysis/main/Load_Data/LoadFunctions.py'
r=requests.get(url)
with open('LoadFunctions.py','w') as f:
  f.write(r.text)

from LoadFunctions import *

DT_s6edge=datapreparation(bukitPanjangToExpo_s6edge, '2020-12-25 09:17:00.000','2020-12-25 10:23:51.000', resize=15, neednormalizepressure = True) #use findErrorDuration2
DT_iphone12pro=datapreparation(bukitPanjangToExpo_iphone12pro,'2020-12-25 09:16:50.000','2020-12-25 10:26:19.000',resize=1, neednormalizepressure = False) #enough 34 stations
DT_Stations = ['DT1_Bukit_Panjang', 'DT2_Cashew', 'DT3_Hillview','DT5_Beauty_World', 'DT6_King_Albert_Park', 'DT7_Sixth_Avenue', 'DT8_Tan_Kah_Kee', 'DT9_Botanic_Gardens', 'DT10_Stevens','DT11_Newton','DT12_Little_India','DT13_Rochor','DT14_Bugis', 'DT15_Promenade','DT16_Bayfront','DT17_Downtown','DT18_Telok Ayer','DT19_Chinatown','DT20_Fort_Canning','DT21_Bencoolen','DT22_Jalan_Besar','DT23_Bendemeer','DT24_Geylang_Bahru','DT25_Mattar','DT26_MacPherson','DT27_Ubi','DT28_Kaki_Bukit','DT29_Bedok_North','DT30_Bedok_Reservoir','DT31_Tampines_West','DT32_Tampines','DT33_Tempines_East','DT34_Upper_Changi','DT35_Expo']

def modeChanged(data):
  totalNumberOfModeChanged = 0
  timestampList = [[data['Mode'][0],data['Time'][0],data['Cleaned_Time'][0]]] #list of list
  timeStampLastMode = data['Cleaned_Time'][0]
  durationOfLastMode = 0
  current_mode = data['Mode'][0]
  for i in range(len(data)):
    mode = data['Mode'][i]
    if mode != current_mode:
      totalNumberOfModeChanged += 1
      duration = data['Cleaned_Time'][i] - timeStampLastMode
      timestampList[-1][2] = duration #only after next change of mode we are able to calculate the duration of last mode, without this the mode and duration output will be interchanged
      
      #insert end time for previous mode as current timestamp
      timestampList[-1].append(data['Time'][i])

      #insert new entries
      timestampList.append([mode,data['Time'][i],0]) #0 is just preparation for adding duration in the next loop

      #update new count
      current_mode = mode
      timeStampLastMode = data['Cleaned_Time'][i]
#  print("Total number of Mode changed is :",totalNumberOfModeChanged)
  return timestampList

def findErrorDuration(timestampList):
  errorCount = 0
  global errorList
  errorList = []
  for item in timestampList:
    if item[2] < 20:
      errorCount+=1
      errorList.append([item[1],item[2]])
  print("Total number of error entries is :",errorCount)
  print(errorList)
  return errorCount, errorList

def removeBackwardTimestampError(modeList):
  newModeList = [modeList[0]]
  removeList = []
  currentTime = modeList[0][1]
  modeList.pop(0)
  while modeList != []:
    if modeList[0][1] <= currentTime:
      removeList.append(modeList[0])
      modeList.remove(modeList[0])
    else:
      newModeList.append(modeList[0])
      currentTime = modeList[0][1]
      newModeList[-2][3] = currentTime
      modeList.remove(modeList[0])
  return newModeList

import datetime
def recalculateDuration(modelist):
  for i in range(len(modelist)-1):
    if isinstance(modelist[0][1],str) == True:
      endtime = datetime.datetime.strptime(modelist[i][3], '%Y-%m-%d %H:%M:%S')
      starttime = datetime.datetime.strptime(modelist[i][1], '%Y-%m-%d %H:%M:%S')
    else:
      endtime = modelist[i][3]
      starttime = modelist[i][1]
    time = endtime - starttime
    modelist[i][2] = time.total_seconds()
  return modelist

def removeErrorDuration(modelist):
  newlist = []
  for i in range(len(modelist)-1):
    if modelist[i][2] < 5: #less than 5 seconds
      pass
    else:
      newlist.append(modelist[i])
  newlist.append(modelist[-1]) #add last item in
  return newlist

def selectTimestampKey(modelist):
  newlist = []
  for i in modelist:
    newlist.append(i[1])
  return newlist

def findRepetitiveMode(modelist):
  repetitivelist = []
  mode = modelist[0][0]
  for i in range(1,len(modelist)):
    if modelist[i][0] == mode:
      print("Repetitve mode error at ",modelist[i])
      repetitivelist.append(modelist[i])
    else:
      mode = modelist[i][0]
  return repetitivelist

def removeRepetitiveMode(modelist, repetitivelist):
  newlist = []
  print("remove starting here!")
  for i in range(len(modelist)-1):
    if modelist[i] in repetitivelist:
      print(modelist[i],"is going to be removed")
      newlist[-1][3] = modelist[i][3]
    else:
      newlist.append(modelist[i])
#  print("newlist length is:", len(newlist))
  return newlist

def checkTimestampIsString(modelist):
  newlist = []
  if isinstance(modelist[0][1], str) == False:
    for i in range(len(modelist)-1):
      modelist[i][1] = str(modelist[i][1])
      modelist[i][3] = str(modelist[i][3])
      newlist.append(modelist[i])
    newlist.append(modelist[-1])
    newlist[-1][1] = str(newlist[-1][1])
    return newlist
  return modelist

def errorRemovingPipeline(data):
  modeChangedList = modeChanged(data)
  print("The length after modeChanged is :", len(modeChangedList))

  newModeChangedList = removeBackwardTimestampError(modeChangedList)
  print("The length after removeBackwardTimestampError is :", len(newModeChangedList))

  recalculatedModeList = recalculateDuration(newModeChangedList)
  print("The length after first recalculated is :", len(recalculatedModeList))

  recalculatedAndRemovedList = removeErrorDuration(recalculatedModeList)

  repetitiveList = findRepetitiveMode(recalculatedAndRemovedList)
  if repetitiveList != []:
    recalculatedAndRemovedList = removeRepetitiveMode(recalculatedAndRemovedList, repetitiveList)

  print("The length after recalculated and repetitive mode removed is :", len(recalculatedAndRemovedList))
  finalList = checkTimestampIsString(recalculatedAndRemovedList)

  finalList[-1].append('0')

  return finalList

def appendStation(modelist,stationlist):
  tempStationList = stationlist[:] #copy without referencing, if not stationlist will get pop and cannot be reused
  for item in modelist:
    if item[0] == "Idle" or item[0] == "PMD": #put PMD because TE line got PMD as Idle
      print(tempStationList[0])
      item.append(tempStationList[0])
      tempStationList.pop(0)
    else:
      item.append("Moving")
  return modelist

def addStationToDf(data,modelist):
  data['Station'] = '0'
  while modelist != []:
    for i in range(len(data['Time'])):
      if str(data['Time'][i]) == modelist[0][1]:
        data['Station'][i] = modelist[0][4]
        print(data['Station'][i],'is added!!!')
        modelist.pop(0)
        break
  return data

def fillEmptyStationToDf(data):
  temp = data['Station'][0]
  for i in range(len(data['Station'])):
    if data['Station'][i] != '0':
      temp = data['Station'][i]
    else:
      data['Station'][i] = temp
  return data

#combine 2 functions together
def modifyColumnStation(data,modelist):
  data1 = addStationToDf(data,modelist)
  print("add station done!!!")
  data2 = fillEmptyStationToDf(data1)
  print("fill empty done!!!")
  return data2

DT_iphone12pro_temp = errorRemovingPipeline(DT_iphone12pro)
DT_iphone12pro_keyvalues = appendStation(DT_iphone12pro_temp,DT_Stations)
DT_iphone12pro = modifyColumnStation(DT_iphone12pro,DT_iphone12pro_keyvalues)

DT_s6edge_temp = errorRemovingPipeline(DT_s6edge)
DT_s6edge_keyvalues = appendStation(DT_s6edge_temp,DT_Stations)
DT_s6edge = modifyColumnStation(DT_s6edge,DT_s6edge_keyvalues)

"""#Start"""

DT_iphone12pro

DT_s6edge

dropmoving_iphone=DT_iphone12pro.loc[DT_iphone12pro['Station']=='Moving']
DT_iphone12pro.drop(DT_iphone12pro.index[list(dropmoving_iphone.index)],inplace=True)
dropmoving_s6edge=DT_s6edge.loc[DT_s6edge['Station']=='Moving']
DT_s6edge.drop(DT_s6edge.index[list(dropmoving_s6edge.index)],inplace=True)

DT_Stations = ['DT1_Bukit_Panjang', 'DT2_Cashew', 'DT3_Hillview','DT5_Beauty_World', 'DT6_King_Albert_Park', 'DT7_Sixth_Avenue', 'DT8_Tan_Kah_Kee', 'DT9_Botanic_Gardens', 'DT10_Stevens','DT11_Newton','DT12_Little_India','DT13_Rochor','DT14_Bugis', 'DT15_Promenade','DT16_Bayfront','DT17_Downtown','DT18_Telok Ayer','DT19_Chinatown','DT20_Fort_Canning','DT21_Bencoolen','DT22_Jalan_Besar','DT23_Bendemeer','DT24_Geylang_Bahru','DT25_Mattar','DT26_MacPherson','DT27_Ubi','DT28_Kaki_Bukit','DT29_Bedok_North','DT30_Bedok_Reservoir','DT31_Tampines_West','DT32_Tampines','DT33_Tempines_East','DT34_Upper_Changi','DT35_Expo']

features = ['Acc_Lin_X', 'Acc_Lin_Y', 'Acc_Lin_Z', 
            'Acc_X', 'Acc_Y', 'Acc_Z', 'Bar_Pressure', 
            'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Mag_X', 'Mag_Y', 'Mag_Z','Station']

MRT_data_iphone = [DT_iphone12pro_movingRemoved[features]]

mrt_data_iphone = pd.concat(MRT_data_iphone)
mrt_data_iphone.reset_index(inplace=True)
mrt_data_iphone = mrt_data_iphone.drop('index', axis=1)

# Fill in 0 for all Null values EXCEPT locations columns (Loc_Altitude, Loc_Latitude, Loc_Longitude)
for i in features:
    mrt_data_iphone[i].fillna(0, inplace=True)

mrt_data_iphone

# Omit the 'Mode' column
mrt_data_features_iphone = mrt_data_iphone[[ 'Acc_X', 'Acc_Y', 'Acc_Z', 'Bar_Pressure', 
                              'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Mag_X', 'Mag_Y', 'Mag_Z']]

# The ground truth
mrt_data_mode_iphone = mrt_data_iphone['Station']

# Normalize the feature values
mrt_data_features_normalized_iphone=(mrt_data_features_iphone-mrt_data_features_iphone.mean())/mrt_data_features_iphone.std()

"""#Clustering"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

data = mrt_data_features_iphone

data.describe()

# standardizing the data
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)

# statistics of scaled data
pd.DataFrame(data_scaled).describe()

# defining the kmeans function with initialization as k-means++
kmeans = KMeans(n_clusters=2, init='k-means++')

# fitting the k means algorithm on scaled data
kmeans.fit(data_scaled)

kmeans.inertia_

SSE = []
for cluster in range(1,20):
    kmeans = KMeans(n_jobs = -1, n_clusters = cluster, init='k-means++')
    kmeans.fit(data_scaled)
    SSE.append(kmeans.inertia_)

# converting the results into a dataframe and plotting them
frame = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})
plt.figure(figsize=(12,6))
plt.plot(frame['Cluster'], frame['SSE'], marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')

# k means using 5 clusters and k-means++ initialization
kmeans = KMeans(n_jobs = -1, n_clusters = 5, init='k-means++')
kmeans.fit(data_scaled)
pred = kmeans.predict(data_scaled)

frame = pd.DataFrame(data_scaled)
frame['cluster'] = pred
frame['cluster'].value_counts()

frame

mrt_data_mode_iphone

mrt_data_iphone['category'] = pred

mrt_data_iphone

group = mrt_data_iphone.groupby('category')

output = group.apply(lambda x: x['Station'].unique())

output

for i in output:
  print(i)

group0 = mrt_data_iphone[mrt_data_iphone['category'] == 0]
group1 = mrt_data_iphone[mrt_data_iphone['category'] == 1]
group2 = mrt_data_iphone[mrt_data_iphone['category'] == 2]
group3 = mrt_data_iphone[mrt_data_iphone['category'] == 3]
group4 = mrt_data_iphone[mrt_data_iphone['category'] == 4]

group2

"""#Visualisation between clusters"""

def visualizationAfterClustering(data1, data2, data3, data4, data5, data6):
  '''visualise the data for each numerical variables after clustering using different colours'''
  '''if less than 6 clusters, put 'None' as parameter'''
  plt.figure(figsize=(20,72))
  for i, col in enumerate(['Acc_Lin_X', 'Acc_Lin_Y', 'Acc_Lin_Z', 'Acc_X', 'Acc_Y','Acc_Z', 'Bar_Pressure', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Mag_X', 'Mag_Y','Mag_Z'],start=1):
    plt.subplot(8,2,i)
    plt.plot(data1[col],'.y',label='Cluster 1')
    if data2 is not None:
      plt.plot(data2[col],'.r',label='Cluster 2')
    if data3 is not None:
      plt.plot(data3[col],'.g',label='Cluster 3')
    if data4 is not None:
      plt.plot(data4[col],'.b',label='Cluster 4')
    if data5 is not None:
      plt.plot(data5[col],'.c',label='Cluster 5')
    if data6 is not None:
      plt.plot(data6[col],'.m',label='Cluster 6')
    plt.title(col)
  plt.legend()
  plt.xlabel('Time') #actually is the index count
  plt.title(col)

visualizationAfterClustering(group0, group1, group2, group3, group4, None)

"""# Silhouette Scores
The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.
"""

from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.cm as cm

def silhouetteAnalysis(data):
  '''get silhouette scores to know the number of clusters to be used'''
  '''used only the dataset with numerical variables as input parameter'''
  range_n_clusters = [2, 3, 4, 5, 6]
  for n_clusters in range_n_clusters:
      # Create a subplot with 1 row and 2 columns
      fig, (ax1, ax2) = plt.subplots(1, 2)
      fig.set_size_inches(18, 7)

      # The 1st subplot is the silhouette plot
      # The silhouette coefficient can range from -1, 1 but in this example all
      # lie within [-0.1, 1]
      ax1.set_xlim([-0.1, 1])
      # The (n_clusters+1)*10 is for inserting blank space between silhouette
      # plots of individual clusters, to demarcate them clearly.
      ax1.set_ylim([0, len(data) + (n_clusters + 1) * 10])

      # Initialize the clusterer with n_clusters value and a random generator
      # seed of 10 for reproducibility.
      clusterer = KMeans(n_clusters=n_clusters, random_state=10)
      cluster_labels = clusterer.fit_predict(data)

      # The silhouette_score gives the average value for all the samples.
      # This gives a perspective into the density and separation of the formed
      # clusters
      silhouette_avg = silhouette_score(data, cluster_labels)
      print("For n_clusters =", n_clusters,
            "The average silhouette_score is :", silhouette_avg)

      # Compute the silhouette scores for each sample
      sample_silhouette_values = silhouette_samples(data, cluster_labels)

      y_lower = 10
      for i in range(n_clusters):
          # Aggregate the silhouette scores for samples belonging to
          # cluster i, and sort them
          ith_cluster_silhouette_values = \
              sample_silhouette_values[cluster_labels == i]

          ith_cluster_silhouette_values.sort()

          size_cluster_i = ith_cluster_silhouette_values.shape[0]
          y_upper = y_lower + size_cluster_i

          color = cm.nipy_spectral(float(i) / n_clusters)
          ax1.fill_betweenx(np.arange(y_lower, y_upper),
                            0, ith_cluster_silhouette_values,
                            facecolor=color, edgecolor=color, alpha=0.7)

          # Label the silhouette plots with their cluster numbers at the middle
          ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

          # Compute the new y_lower for next plot
          y_lower = y_upper + 10  # 10 for the 0 samples

      ax1.set_title("The silhouette plot for the various clusters.")
      ax1.set_xlabel("The silhouette coefficient values")
      ax1.set_ylabel("Cluster label")

      # The vertical line for average silhouette score of all the values
      ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

      ax1.set_yticks([])  # Clear the yaxis labels / ticks
      ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

      # 2nd Plot showing the actual clusters formed
      colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
      ax2.scatter(data[:, 0], data[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                  c=colors, edgecolor='k')

      # Labeling the clusters
      centers = clusterer.cluster_centers_
      # Draw white circles at cluster centers
      ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                  c="white", alpha=1, s=200, edgecolor='k')

      for i, c in enumerate(centers):
          ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                      s=50, edgecolor='k')

      ax2.set_title("The visualization of the clustered data.")
      ax2.set_xlabel("Feature space for the 1st feature")
      ax2.set_ylabel("Feature space for the 2nd feature")

      plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                    "with n_clusters = %d" % n_clusters),
                  fontsize=14, fontweight='bold')

  plt.show()

data_scaled

silhouetteAnalysis(data_scaled)



"""# Clustering on combined iPhone and S6edge data for Downtown line"""

features = ['Mode', 'Acc_Lin_X', 'Acc_Lin_Y', 'Acc_Lin_Z', 
            'Acc_X', 'Acc_Y', 'Acc_Z', 'Bar_Pressure', 
            'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Mag_X', 'Mag_Y', 'Mag_Z','Station']

MRT_data_iphone = [DT_iphone12pro[features]]

mrt_data_iphone = pd.concat(MRT_data_iphone)
mrt_data_iphone.reset_index(inplace=True)
mrt_data_iphone = mrt_data_iphone.drop('index', axis=1)

MRT_data_s6edge = [DT_s6edge[features]]

mrt_data_s6edge = pd.concat(MRT_data_s6edge)
mrt_data_s6edge.reset_index(inplace=True)
mrt_data_s6edge = mrt_data_s6edge.drop('index', axis=1)
for i in features:
    mrt_data_iphone[i].fillna(0, inplace=True) #can try fill with mean?
    mrt_data_s6edge[i].fillna(0, inplace=True)

mrt_data_features_iphone = mrt_data_iphone[[ 'Acc_Lin_X', 'Acc_Lin_Y', 'Acc_Lin_Z', 
            'Acc_X', 'Acc_Y', 'Acc_Z', 'Bar_Pressure', 
            'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Mag_X', 'Mag_Y', 'Mag_Z']]
mrt_data_features_s6edge = mrt_data_s6edge[[ 'Acc_Lin_X', 'Acc_Lin_Y', 'Acc_Lin_Z', 
            'Acc_X', 'Acc_Y', 'Acc_Z', 'Bar_Pressure', 
            'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Mag_X', 'Mag_Y', 'Mag_Z']]
combined_data=pd.concat([mrt_data_features_iphone,mrt_data_features_s6edge])
mrt_data_combined=pd.concat([DT_iphone12pro,DT_s6edge])

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
# standardizing the data
scaler = StandardScaler()
data_scaled = scaler.fit_transform(combined_data)

# statistics of scaled data
pd.DataFrame(data_scaled).describe()

pd.DataFrame(combined_data).isnull().sum()

# defining the kmeans function with initialization as k-means++
kmeans = KMeans(n_clusters=2, init='k-means++')

# fitting the k means algorithm on scaled data
kmeans.fit(data_scaled)

# k means using 5 clusters and k-means++ initialization
kmeans = KMeans(n_jobs = -1, n_clusters = 4, init='k-means++')
kmeans.fit(data_scaled)
pred = kmeans.predict(data_scaled)

frame = pd.DataFrame(data_scaled)
frame['cluster'] = pred
frame['cluster'].value_counts()

mrt_data_combined['category'] = pred

group = mrt_data_combined.groupby('category')

output = group.apply(lambda x: x['Station'].unique())

for i in output:
    print(i)

group0_combined = mrt_data_combined[mrt_data_combined['category'] == 0]
group1_combined = mrt_data_combined[mrt_data_combined['category'] == 1]
group2_combined = mrt_data_combined[mrt_data_combined['category'] == 2]
group3_combined = mrt_data_combined[mrt_data_combined['category'] == 3]
for i in features:
    group0_combined[i].fillna(0, inplace=True) #can try fill with mean?
    group1_combined[i].fillna(0, inplace=True)
    group2_combined[i].fillna(0, inplace=True)
    group3_combined[i].fillna(0, inplace=True)

visualizationAfterClustering(group0_combined,group1_combined,group2_combined,None,None,None)

mrt_data_combined

mrt_data_combined_features = mrt_data_combined[[ 'Acc_X', 'Acc_Y', 'Acc_Z', 'Bar_Pressure', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Mag_X', 'Mag_Y', 'Mag_Z']]
mrt_data_combined_features

features2 = ['Acc_X', 'Acc_Y', 'Acc_Z', 'Bar_Pressure', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Mag_X', 'Mag_Y', 'Mag_Z']

for i in features2:
    mrt_data_combined_features[i].fillna(0, inplace=True)

mrt_data_combined_features

#next step has to be array and no NaN

silhouetteAnalysis(mrt_data_combined_features.values)

group0_combined

"""# Random forest to predict stations based on clusters"""

RFC    = RandomForestClassifier(n_estimators=10, max_depth=50)
def randomforest_stationprediction_cluster(group):
    cfeatures = group[[ 'Acc_Z', 'Bar_Pressure','Mag_X', 'Acc_Y', 'Mag_Z']]

    # The ground truth
    cmode = group['Station']

    # Normalize the feature values
    cfeatures_normalized=(cfeatures-cfeatures.mean())/cfeatures.std()
    
    x_train, x_test, y_train, y_test = train_test_split(cfeatures_normalized, cmode, test_size=0.3)
    
    RFC.fit(x_train, y_train)

    rfc_predict  = RFC.predict(x_test)

    # Confusion matrix
    print("=== Confusion Matrix ===")
    print(confusion_matrix(y_test, rfc_predict))
    print('\n')

    # Classification Report
    print("=== Classification Report ===")
    print(classification_report(y_test, rfc_predict))
    print('\n')

"""## Group 0"""

randomforest_stationprediction_cluster(group0_combined)

"""## Group 1"""

randomforest_stationprediction_cluster(group1_combined)

"""## Group 2"""

randomforest_stationprediction_cluster(group2_combined)

"""## Group 3"""

randomforest_stationprediction_cluster(group3_combined)